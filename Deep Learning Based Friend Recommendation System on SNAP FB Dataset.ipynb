{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复杂网络课程设计 Complex Network Course Design\n",
    "# Deep Learning Based Friend Recommendation System on SNAP Higgs-Twitter Dataset \n",
    "# by [戴迪康220245507 & 徐敬逸220245432]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ov2189j0nmt-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import seaborn as sns\n",
    "from scipy.sparse import lil_matrix,csr_matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,precision_score,recall_score,log_loss,roc_auc_score,classification_report,confusion_matrix\n",
    "from sklearn.model_selection import KFold,cross_val_score,train_test_split\n",
    "from sklearn import svm,linear_model,metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import normalize,StandardScaler\n",
    "from sklearn.utils import resample, class_weight, shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP8mBd_ZommL"
   },
   "source": [
    "### **Visualizing  the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCjV2cTzn3gV",
    "outputId": "f575399b-901a-4f85-e35c-3c27327c42cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000033"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load edges (or links)\n",
    "warnings.filterwarnings('ignore')\n",
    "with open(\"/home/dikangdai/1_my_project/Twitter.txt\") as f:\n",
    "    fb_links = f.read().splitlines() \n",
    "len(fb_links) #返回fb_links列表的长度，即文件中边的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZQR8QbcFn6o6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  node_1 node_2\n",
       "0      1      2\n",
       "1      1      3\n",
       "2      1      4\n",
       "3      1      5\n",
       "4      1      6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取出每一条边的两个节点，存储在两个n1,n2列表中，并转换为dataframe的‘node_1’和‘node_2’两列\n",
    "n1 = []\n",
    "n2 = []\n",
    "\n",
    "for i in fb_links:\n",
    "  n1.append(i.split('\\t')[0])\n",
    "  n2.append(i.split('\\t')[1])\n",
    "\n",
    "# fb_df 是一个包含两个列 (node_1 和 node_2) 的 DataFrame，代表网络中的每一条边，其中 node_1 和 node_2 分别是边的两个节点\n",
    "fb_df = pd.DataFrame({'node_1': n1, 'node_2': n2})\n",
    "fb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "id": "klnHXYCeoEmB",
    "outputId": "9f2fbd79-0e21-4c1d-dce4-5030dfe7cf0a"
   },
   "outputs": [],
   "source": [
    "# 可视化数据集\n",
    "G = nx.from_pandas_edgelist(fb_df, \"node_1\", \"node_2\", create_using=nx.Graph())\n",
    "# plt.figure(figsize=(10,10))\n",
    "# nx.draw(G, with_labels=False, node_size = 20, alpha = 0.6, width = 0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mz8d5kts-kyj",
    "outputId": "fd87fd1f-6d53-4ae1-e44b-00f86465a9a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134585"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMCIX8LqpAJh"
   },
   "source": [
    "### **Creating the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FGnrgv_JoJjm"
   },
   "outputs": [],
   "source": [
    "# 找到数据集中所有的唯一节点（不重复的节点）\n",
    "nl = n1 + n2\n",
    "nl = list(dict.fromkeys(nl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将图 G 转换为稀疏矩阵\n",
    "adj_G = nx.to_scipy_sparse_matrix(G, nodelist=nl, format=\"csr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nN_NNC8ZoOz8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "负样本生成进度:   0%|          | 16/50000 [00:00<05:12, 159.75样本/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始生成负样本数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "负样本生成进度: 100%|██████████| 50000/50000 [05:28<00:00, 152.26样本/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "负样本数据生成完成，共生成 49999 个负样本.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 生成负样本\n",
    "# 找出不相连但距离小于等于 2 的节点对\n",
    "all_unconnected_pairs = []\n",
    "num_samples=50000 # 生成负样本个数\n",
    "\n",
    "# # 将稀疏矩阵转换为 COO 格式，方便遍历非零元素\n",
    "# adj_G_coo = adj_G.tocoo()\n",
    "\n",
    "print(\"开始生成负样本数据...\")\n",
    "for _ in tqdm(range(num_samples), desc=\"负样本生成进度\", unit=\"样本\"):\n",
    "    u, v = np.random.choice(range(len(nl)), 2, replace=False)\n",
    "    if adj_G[u, v] == 0 and G.has_edge(nl[u], nl[v]) == False:\n",
    "            all_unconnected_pairs.append([nl[u], nl[v]])\n",
    "print(f\"负样本数据生成完成，共生成 {len(all_unconnected_pairs)} 个负样本.\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zh7Rw3jZoTFj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67657</td>\n",
       "      <td>29113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41052</td>\n",
       "      <td>111474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61285</td>\n",
       "      <td>114560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33644</td>\n",
       "      <td>98271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42151</td>\n",
       "      <td>97868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  node_1  node_2  link\n",
       "0  67657   29113     0\n",
       "1  41052  111474     0\n",
       "2  61285  114560     0\n",
       "3  33644   98271     0\n",
       "4  42151   97868     0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_1_unlinked = [i[0] for i in all_unconnected_pairs]\n",
    "node_2_unlinked = [i[1] for i in all_unconnected_pairs]\n",
    "\n",
    "negative_samples = pd.DataFrame({'node_1':node_1_unlinked, \n",
    "                     'node_2':node_2_unlinked})\n",
    "negative_samples['link'] = 0\n",
    "\n",
    "negative_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  node_1 node_2  link\n",
       "0      1      2     1\n",
       "1      1      3     1\n",
       "2      1      4     1\n",
       "3      1      5     1\n",
       "4      1      6     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从图中提取正样本（已经有连接的节点对）\n",
    "\n",
    "all_connected_pairs = []\n",
    "\n",
    "# 遍历所有已连接的节点对\n",
    "for u, v in list(G.edges()):\n",
    "    u_neighbors = set(G.neighbors(u))  # 获取节点 u 的邻居\n",
    "    v_neighbors = set(G.neighbors(v))  # 获取节点 v 的邻居\n",
    "\n",
    "    # 如果节点 u 和节点 v 有公共邻居\n",
    "    if u_neighbors.intersection(v_neighbors):\n",
    "        # print(len(u_neighbors.intersection(v_neighbors)),\" / \",len(u_neighbors.union(v_neighbors)))\n",
    "        all_connected_pairs.append((u, v))\n",
    "\n",
    "\n",
    "node_1_linked = [i[0] for i in all_connected_pairs]\n",
    "node_2_linked = [i[1] for i in all_connected_pairs]\n",
    "\n",
    "positive_samples = pd.DataFrame({'node_1':node_1_linked, \n",
    "                                 'node_2':node_2_linked})\n",
    "positive_samples['link'] = 1 # 连接的样本标签为 1\n",
    "\n",
    "positive_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjDK-oiKpqHO"
   },
   "source": [
    "### **Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddKZwi03tjoC",
    "outputId": "20852682-a59a-4ee1-ed5f-b796e2b43dc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660446, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义五种特征\n",
    "def JaccardCoefficient(u, v, g):\n",
    "    u_neighbors = set(g.neighbors(u))\n",
    "    v_neighbors = set(g.neighbors(v))\n",
    "    lenth_intersection = len(u_neighbors.intersection(v_neighbors))\n",
    "    lenth_union = len(u_neighbors.union(v_neighbors))\n",
    "    return lenth_intersection / lenth_union\n",
    "\n",
    "\n",
    "def PreferentialAttachment(u, v, g):\n",
    "    return len(list(g.neighbors(u))) * len(list(g.neighbors(v)))\n",
    "\n",
    "def AdamicAdar(u, v, g):\n",
    "    common_neighbors = set(g.neighbors(u)).intersection(g.neighbors(v))\n",
    "    adamic_adar_index = 0.0\n",
    "    for w in common_neighbors:\n",
    "        neighbor_count = len(list(g.neighbors(w)))\n",
    "        if neighbor_count > 1:  # 避免 log(1) = 0\n",
    "            adamic_adar_index += 1 / np.log(neighbor_count)\n",
    "    return adamic_adar_index\n",
    "\n",
    "def CommonNeighbors(u, v, g):\n",
    "    u_neighbors = set(g.neighbors(u))\n",
    "    v_neighbors = set(g.neighbors(v))\n",
    "    return len(u_neighbors.intersection(v_neighbors))\n",
    "\n",
    "def ResourceAllocation(u, v, g):\n",
    "    common_neighbors = set(g.neighbors(u)).intersection(g.neighbors(v))\n",
    "    resource_allocation_index = 0.0\n",
    "    for w in common_neighbors:\n",
    "        resource_allocation_index += 1 / len(list(g.neighbors(w)))\n",
    "    return resource_allocation_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "正样本特征计算:   0%|          | 55/660446 [00:00<20:24, 539.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始计算正负样本特征...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "正样本特征计算: 100%|██████████| 660446/660446 [01:44<00:00, 6296.49it/s] \n",
      "负样本特征计算: 100%|██████████| 49999/49999 [00:01<00:00, 30660.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正负样本特征计算完毕！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### 计算正负样本的特征\n",
    "\n",
    "print(\"开始计算正负样本特征...\")\n",
    "\n",
    "# 正样本\n",
    "for row in tqdm(positive_samples.itertuples(), desc=\"正样本特征计算\", total=len(positive_samples)):\n",
    "    u, v = row.node_1, row.node_2\n",
    "    positive_samples.at[row.Index, 'JaccardCoefficient'] = JaccardCoefficient(u, v, G)\n",
    "    positive_samples.at[row.Index, 'PreferentialAttachment'] = PreferentialAttachment(u, v, G)\n",
    "    positive_samples.at[row.Index, 'AdamicAdar'] = AdamicAdar(u, v, G)\n",
    "    positive_samples.at[row.Index, 'CommonNeighbors'] = CommonNeighbors(u, v, G)\n",
    "    positive_samples.at[row.Index, 'ResourceAllocation'] = ResourceAllocation(u, v, G)\n",
    "\n",
    "# 负样本\n",
    "for row in tqdm(negative_samples.itertuples(), desc=\"负样本特征计算\", total=len(negative_samples)):\n",
    "    u, v = row.node_1, row.node_2\n",
    "    negative_samples.at[row.Index, 'JaccardCoefficient'] = JaccardCoefficient(u, v, G)\n",
    "    negative_samples.at[row.Index, 'PreferentialAttachment'] = PreferentialAttachment(u, v, G)\n",
    "    negative_samples.at[row.Index, 'AdamicAdar'] = AdamicAdar(u, v, G)\n",
    "    negative_samples.at[row.Index, 'CommonNeighbors'] = CommonNeighbors(u, v, G)\n",
    "    negative_samples.at[row.Index, 'ResourceAllocation'] = ResourceAllocation(u, v, G)\n",
    "\n",
    "print(\"正负样本特征计算完毕！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>link</th>\n",
       "      <th>JaccardCoefficient</th>\n",
       "      <th>PreferentialAttachment</th>\n",
       "      <th>AdamicAdar</th>\n",
       "      <th>CommonNeighbors</th>\n",
       "      <th>ResourceAllocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67657</td>\n",
       "      <td>29113</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41052</td>\n",
       "      <td>111474</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61285</td>\n",
       "      <td>114560</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33644</td>\n",
       "      <td>98271</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42151</td>\n",
       "      <td>97868</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  node_1  node_2  link  JaccardCoefficient  PreferentialAttachment  \\\n",
       "0  67657   29113     0                 0.0                     1.0   \n",
       "1  41052  111474     0                 0.0                     3.0   \n",
       "2  61285  114560     0                 0.0                    63.0   \n",
       "3  33644   98271     0                 0.0                    73.0   \n",
       "4  42151   97868     0                 0.0                     1.0   \n",
       "\n",
       "   AdamicAdar  CommonNeighbors  ResourceAllocation  \n",
       "0         0.0              0.0                 0.0  \n",
       "1         0.0              0.0                 0.0  \n",
       "2         0.0              0.0                 0.0  \n",
       "3         0.0              0.0                 0.0  \n",
       "4         0.0              0.0                 0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>link</th>\n",
       "      <th>JaccardCoefficient</th>\n",
       "      <th>PreferentialAttachment</th>\n",
       "      <th>AdamicAdar</th>\n",
       "      <th>CommonNeighbors</th>\n",
       "      <th>ResourceAllocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>162687.0</td>\n",
       "      <td>1.750585</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.047733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>11346.0</td>\n",
       "      <td>0.305213</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.003120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>396378.0</td>\n",
       "      <td>3.141461</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.118624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>100650.0</td>\n",
       "      <td>1.054282</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.012897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>540765.0</td>\n",
       "      <td>2.298217</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.066808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  node_1 node_2  link  JaccardCoefficient  PreferentialAttachment  AdamicAdar  \\\n",
       "0      1      2     1            0.009416                162687.0    1.750585   \n",
       "1      1      3     1            0.008230                 11346.0    0.305213   \n",
       "2      1      4     1            0.007290                396378.0    3.141461   \n",
       "3      1      5     1            0.009642                100650.0    1.054282   \n",
       "4      1      6     1            0.004160                540765.0    2.298217   \n",
       "\n",
       "   CommonNeighbors  ResourceAllocation  \n",
       "0             10.0            0.047733  \n",
       "1              2.0            0.003120  \n",
       "2             17.0            0.118624  \n",
       "3              7.0            0.012897  \n",
       "4             13.0            0.066808  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660446, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bFWiKY_v4LZY"
   },
   "outputs": [],
   "source": [
    "negative_samples.to_csv('negative_samples.csv')\n",
    "positive_samples.to_csv('positive_samples.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Datasets Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正样本数量: (660446, 9)\n",
      "负样本数量: (1460, 9)\n",
      "非零特征的负样本数量: (1460, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 从生成的正负样本数据集中，加载数据\n",
    "positive_df = pd.read_csv('positive_samples_1.csv')\n",
    "negative_df = pd.read_csv('non_zero_negative_samples_1.csv')\n",
    "\n",
    "# 查看数据概况\n",
    "print(\"正样本数量:\", positive_df.shape)\n",
    "print(\"负样本数量:\", negative_df.shape)\n",
    "\n",
    "# 筛选负样本中，五个特征值均不为零的样本数量\n",
    "non_zero_negatives = negative_df[(negative_df[['JaccardCoefficient', 'PreferentialAttachment', 'AdamicAdar', 'CommonNeighbors', 'ResourceAllocation']] != 0.0).all(axis=1)]\n",
    "print(\"非零特征的负样本数量:\", non_zero_negatives.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据异常值检查情况：\n",
      "Unnamed: 0                0\n",
      "node_1                    0\n",
      "node_2                    0\n",
      "link                      0\n",
      "JaccardCoefficient        0\n",
      "PreferentialAttachment    0\n",
      "AdamicAdar                0\n",
      "CommonNeighbors           0\n",
      "ResourceAllocation        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 合并数据集\n",
    "data_df = pd.concat([positive_df, non_zero_negatives], ignore_index=True)\n",
    "\n",
    "# 检查缺失值\n",
    "print(\"数据异常值检查情况：\")\n",
    "print(data_df.isnull().sum())\n",
    "\n",
    "# # 如果存在缺失值，可以选择填充或删除\n",
    "# data_df = data_df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tpr_fpr(conf_matrix):\n",
    "    # 从混淆矩阵中提取 TP, FN, FP, TN\n",
    "    TP = conf_matrix[1, 1]\n",
    "    FN = conf_matrix[1, 0]\n",
    "    FP = conf_matrix[0, 1]\n",
    "    TN = conf_matrix[0, 0]\n",
    "    \n",
    "    # 计算 TPR 和 FPR\n",
    "    tpr = TP / (TP + FN)  # True Positive Rate\n",
    "    fpr = FP / (FP + TN)  # False Positive Rate\n",
    "    \n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集标签分布:\n",
      " link\n",
      "1    462312\n",
      "0      1022\n",
      "Name: count, dtype: int64\n",
      "测试集标签分布:\n",
      " link\n",
      "1    198134\n",
      "0       438\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  分离特征和标签\n",
    "feature_cols = ['JaccardCoefficient', 'PreferentialAttachment', 'AdamicAdar', 'CommonNeighbors', 'ResourceAllocation']\n",
    "\n",
    "X = data_df[feature_cols]\n",
    "y = data_df['link']\n",
    "\n",
    "# ------------- 划分数据集 -------------\n",
    "# 先划分训练集与测试集，确保测试集的真实性\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "print(\"训练集标签分布:\\n\", y_train.value_counts())\n",
    "print(\"测试集标签分布:\\n\", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分开正样本和负样本\n",
    "x_train_pos = x_train[y_train == 1]\n",
    "y_train_pos = y_train[y_train == 1]\n",
    "x_train_neg = x_train[y_train == 0]\n",
    "y_train_neg = y_train[y_train == 0]\n",
    "\n",
    "# 数据平衡操作：对训练集的正样本进行随机欠采样\n",
    "x_train_pos_downsampled, y_train_pos_downsampled = resample(\n",
    "    x_train_pos, y_train_pos,\n",
    "    replace = False,  # 不允许重复采样\n",
    "    n_samples = len(x_train_neg),\n",
    "    random_state = 35\n",
    ")\n",
    "\n",
    "# 合并欠采样后的正样本和原始负样本\n",
    "x_train_balanced = np.vstack((x_train_pos_downsampled, x_train_neg))\n",
    "y_train_balanced = np.hstack((y_train_pos_downsampled, y_train_neg))\n",
    "\n",
    "# # 分开正样本和负样本\n",
    "# x_test_pos = x_test[y_test == 1]\n",
    "# y_test_pos = y_test[y_test == 1]\n",
    "# x_test_neg = x_test[y_test == 0]\n",
    "# y_test_neg = y_test[y_test == 0]\n",
    "\n",
    "# # 数据平衡操作：对测试集的正样本进行随机欠采样\n",
    "# x_test_pos_downsampled, y_test_pos_downsampled = resample(\n",
    "#     x_test_pos, y_test_pos,\n",
    "#     replace = False,  # 不允许重复采样\n",
    "#     n_samples = len(x_test_neg),\n",
    "#     random_state = 35\n",
    "# )\n",
    "\n",
    "# # 合并欠采样后的正样本和原始负样本\n",
    "# x_test_balanced = np.vstack((x_test_pos_downsampled, x_test_neg))\n",
    "# y_test_balanced = np.hstack((y_test_pos_downsampled, y_test_neg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p4hE48xtBA3"
   },
   "source": [
    "### **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9309Y6QuG0F"
   },
   "source": [
    "Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIvWLuqmt-cQ",
    "outputId": "77ae8ef9-8396-4469-8346-0b8ca1de7810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练逻辑回归模型...\n",
      "完成训练逻辑回归模型...\n",
      "逻辑回归模型评估：\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.90      0.01       438\n",
      "           1       1.00      0.65      0.79    198134\n",
      "\n",
      "    accuracy                           0.65    198572\n",
      "   macro avg       0.50      0.78      0.40    198572\n",
      "weighted avg       1.00      0.65      0.79    198572\n",
      "\n",
      "ROC-AUC Score: 0.8907724307515144\n",
      "LR confusion_matrix is:\n",
      "[[   394     44]\n",
      " [ 68606 129528]]\n",
      "Logistic Regression - TPR: 0.6537, FPR: 0.1005\n"
     ]
    }
   ],
   "source": [
    "print(\"开始训练逻辑回归模型...\")\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr.fit(x_train_balanced, y_train_balanced)\n",
    "print (\"完成训练逻辑回归模型...\")\n",
    "lr_result = lr.predict(x_test)\n",
    "score=lr.score(x_test,y_test)\n",
    "accuracy = accuracy_score(lr_result, y_test)\n",
    "lr_cm=metrics.confusion_matrix(y_test, lr_result) #混淆矩阵\n",
    "\n",
    "print(\"逻辑回归模型评估：\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, lr_result))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, lr.predict_proba(x_test)[:, 1]))\n",
    "print(\"LR confusion_matrix is:\")\n",
    "print(lr_cm) #打印混淆矩阵\n",
    "lr_tpr, lr_fpr = calculate_tpr_fpr(lr_cm)\n",
    "print(f\"Logistic Regression - TPR: {lr_tpr:.4f}, FPR: {lr_fpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "lT6oDEOMuMBS",
    "outputId": "b79cd6f6-466d-4117-9e02-d48c2c82fef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片已保存\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(lr_cm, annot=True, fmt=\".1f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);\n",
    "\n",
    "plt.savefig(\"lr_heatmap.png\", format=\"png\")\n",
    "print(\"图片已保存\")\n",
    "plt.close()  # 关闭图形，防止占用内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMqeoudcumN9"
   },
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "8BUbekX9ulHc",
    "outputId": "5bf2f7fd-6352-49e2-d4f4-cfc8cb5f94df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练SVM模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVM训练进度: 100%|██████████| 1/1 [00:00<00:00,  2.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成训练SVM模型...\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.86      0.01       438\n",
      "           1       1.00      0.46      0.63    198134\n",
      "\n",
      "    accuracy                           0.47    198572\n",
      "   macro avg       0.50      0.66      0.32    198572\n",
      "weighted avg       1.00      0.47      0.63    198572\n",
      "\n",
      "ROC-AUC Score: 0.764623336413671\n",
      "SVM confusion_matrix is:\n",
      "[[   377     61]\n",
      " [106144  91990]]\n",
      "SVM - TPR: 0.4643, FPR: 0.1393\n"
     ]
    }
   ],
   "source": [
    "print(\"开始训练SVM模型...\")\n",
    "clf1 = svm.SVC(probability=True,random_state=35)\n",
    "# 分批次训练\n",
    "n_batches = 10  # 设定批次数量\n",
    "batch_size = len(x_train_balanced) // n_batches  # 每个批次的大小\n",
    "\n",
    "# 使用tqdm显示进度\n",
    "for i in tqdm(range(1), desc=\"SVM训练进度\", unit=\"batch\"):\n",
    "    clf1.fit(x_train_balanced, y_train_balanced)\n",
    "\n",
    "print (\"完成训练SVM模型...\")\n",
    "svm_result = clf1.predict(x_test)\n",
    "score=clf1.score(x_test,y_test)\n",
    "svm_cm=metrics.confusion_matrix(y_test, svm_result)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, svm_result))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, clf1.predict_proba(x_test)[:, 1]))\n",
    "print(\"SVM confusion_matrix is:\")\n",
    "print(svm_cm) #打印混淆矩阵\n",
    "svm_tpr, svm_fpr = calculate_tpr_fpr(svm_cm)\n",
    "print(f\"SVM - TPR: {svm_tpr:.4f}, FPR: {svm_fpr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片已保存\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(svm_cm, annot=True, fmt=\".1f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);\n",
    "\n",
    "plt.savefig(\"svm_heatmap.png\", format=\"png\")\n",
    "print(\"图片已保存\")\n",
    "plt.close()  # 关闭图形，防止占用内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练RF模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "随机森林训练进度: 100%|██████████| 1/1 [00:00<00:00,  4.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成训练RF模型...\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.80      0.02       438\n",
      "           1       1.00      0.80      0.89    198134\n",
      "\n",
      "    accuracy                           0.80    198572\n",
      "   macro avg       0.50      0.80      0.45    198572\n",
      "weighted avg       1.00      0.80      0.89    198572\n",
      "\n",
      "ROC-AUC Score: 0.8849899701198484\n",
      "RF confusion_matrix is:\n",
      "[[   352     86]\n",
      " [ 39741 158393]]\n",
      "Random Forest - TPR: 0.7994, FPR: 0.1963\n"
     ]
    }
   ],
   "source": [
    "print(\"开始训练RF模型...\")\n",
    "rf = RandomForestClassifier(random_state=35)\n",
    "# 使用tqdm来显示训练进度\n",
    "for i in tqdm(range(1), desc=\"随机森林训练进度\", unit=\"batch\"):\n",
    "    rf.fit(x_train_balanced, y_train_balanced)\n",
    "print (\"完成训练RF模型...\")\n",
    "rf_result = rf.predict(x_test)\n",
    "rf_cm=metrics.confusion_matrix(y_test, rf_result) #混淆矩阵\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, rf_result))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, rf.predict_proba(x_test)[:, 1]))\n",
    "print(\"RF confusion_matrix is:\")\n",
    "print(rf_cm) #打印混淆矩阵\n",
    "rf_tpr, rf_fpr = calculate_tpr_fpr(rf_cm)\n",
    "print(f\"Random Forest - TPR: {rf_tpr:.4f}, FPR: {rf_fpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片已保存\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(rf_cm, annot=True, fmt=\".1f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);\n",
    "\n",
    "plt.savefig(\"rf_heatmap.png\", format=\"png\")\n",
    "print(\"图片已保存\")\n",
    "plt.close()  # 关闭图形，防止占用内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练KNN模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KNN训练进度: 100%|██████████| 1/1 [00:00<00:00, 561.79batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成训练KNN模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.65      0.01       438\n",
      "           1       1.00      0.65      0.79    198134\n",
      "\n",
      "    accuracy                           0.65    198572\n",
      "   macro avg       0.50      0.65      0.40    198572\n",
      "weighted avg       1.00      0.65      0.79    198572\n",
      "\n",
      "ROC-AUC Score: 0.716449692526247\n",
      "KNN confusion_matrix:\n",
      "[[   283    155]\n",
      " [ 69394 128740]]\n",
      "KNN - TPR: 0.6498, FPR: 0.3539\n"
     ]
    }
   ],
   "source": [
    "print(\"开始训练KNN模型...\")\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "# 使用tqdm来显示训练进度\n",
    "for i in tqdm(range(1), desc=\"KNN训练进度\", unit=\"batch\"):\n",
    "    knn.fit(x_train_balanced, y_train_balanced)\n",
    "print(\"完成训练KNN模型...\")\n",
    "knn_result = knn.predict(x_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, knn_result))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, knn.predict_proba(x_test)[:, 1]))\n",
    "knn_cm = confusion_matrix(y_test, knn_result)\n",
    "print(\"KNN confusion_matrix:\")\n",
    "print(knn_cm)\n",
    "knn_tpr, knn_fpr = calculate_tpr_fpr(knn_cm)\n",
    "print(f\"KNN - TPR: {knn_tpr:.4f}, FPR: {knn_fpr:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(knn_cm, annot=True, fmt=\".1f\", linewidths=.5, square=True, cmap='Blues_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title(f'KNN Accuracy: {accuracy_score(y_test, knn_result):.4f}')\n",
    "plt.savefig(\"knn_heatmap.png\", format=\"png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练XGBoost模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGBoost训练进度: 100%|██████████| 1/1 [00:00<00:00, 22.30batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成训练XGBoost模型...\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.79      0.02       438\n",
      "           1       1.00      0.79      0.88    198134\n",
      "\n",
      "    accuracy                           0.79    198572\n",
      "   macro avg       0.50      0.79      0.45    198572\n",
      "weighted avg       1.00      0.79      0.88    198572\n",
      "\n",
      "ROC-AUC Score: 0.8806304948456775\n",
      "XGBoost confusion_matrix:\n",
      "[[   345     93]\n",
      " [ 41164 156970]]\n",
      "XGBoost - TPR: 0.7922, FPR: 0.2123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"开始训练XGBoost模型...\")\n",
    "xgb_model = xgb.XGBClassifier(random_state=35)\n",
    "# 使用tqdm来显示训练进度\n",
    "for i in tqdm(range(1), desc=\"XGBoost训练进度\", unit=\"batch\"):\n",
    "    xgb_model.fit(x_train_balanced, y_train_balanced)\n",
    "print(\"完成训练XGBoost模型...\")\n",
    "xgb_result = xgb_model.predict(x_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, xgb_result))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, xgb_model.predict_proba(x_test)[:, 1]))\n",
    "xgb_cm = confusion_matrix(y_test, xgb_result)\n",
    "print(\"XGBoost confusion_matrix:\")\n",
    "print(xgb_cm)\n",
    "xgb_tpr, xgb_fpr = calculate_tpr_fpr(xgb_cm)\n",
    "print(f\"XGBoost - TPR: {xgb_tpr:.4f}, FPR: {xgb_fpr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(xgb_cm, annot=True, fmt=\".1f\", linewidths=.5, square=True, cmap='Blues_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title(f'XGBoost Accuracy: {accuracy_score(y_test, xgb_result):.4f}')\n",
    "plt.savefig(\"xgb_heatmap.png\", format=\"png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练LightGBM模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM训练进度: 100%|██████████| 1/1 [00:00<00:00, 26.68batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成训练LightGBM模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.81      0.02       438\n",
      "           1       1.00      0.79      0.88    198134\n",
      "\n",
      "    accuracy                           0.79    198572\n",
      "   macro avg       0.50      0.80      0.45    198572\n",
      "weighted avg       1.00      0.79      0.88    198572\n",
      "\n",
      "ROC-AUC Score: 0.886720868257924\n",
      "LightGBM confusion_matrix:\n",
      "[[   353     85]\n",
      " [ 40819 157315]]\n",
      "LightGBM - TPR: 0.7940, FPR: 0.1941\n"
     ]
    }
   ],
   "source": [
    "print(\"开始训练LightGBM模型...\")\n",
    "lgb_model = lgb.LGBMClassifier(random_state=35,verbose=-1)\n",
    "# 使用tqdm来显示训练进度\n",
    "for i in tqdm(range(1), desc=\"LightGBM训练进度\", unit=\"batch\"):\n",
    "    lgb_model.fit(x_train_balanced, y_train_balanced)\n",
    "print(\"完成训练LightGBM模型...\")\n",
    "lgb_result = lgb_model.predict(x_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, lgb_result))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, lgb_model.predict_proba(x_test)[:, 1]))\n",
    "lgb_cm = confusion_matrix(y_test, lgb_result)\n",
    "print(\"LightGBM confusion_matrix:\")\n",
    "print(lgb_cm)\n",
    "lgb_tpr, lgb_fpr = calculate_tpr_fpr(lgb_cm)\n",
    "print(f\"LightGBM - TPR: {lgb_tpr:.4f}, FPR: {lgb_fpr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(lgb_cm, annot=True, fmt=\".1f\", linewidths=.5, square=True, cmap='Blues_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title(f'LightGBM Accuracy: {accuracy_score(y_test, lgb_result):.4f}')\n",
    "plt.savefig(\"lgb_heatmap.png\", format=\"png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练梯度提升模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gradient Boosting训练进度: 100%|██████████| 1/1 [00:00<00:00,  4.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成训练梯度提升模型...\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.82      0.02       438\n",
      "           1       1.00      0.81      0.89    198134\n",
      "\n",
      "    accuracy                           0.81    198572\n",
      "   macro avg       0.50      0.81      0.46    198572\n",
      "weighted avg       1.00      0.81      0.89    198572\n",
      "\n",
      "ROC-AUC Score: 0.897089341270953\n",
      "Gradient Boosting confusion_matrix:\n",
      "[[   358     80]\n",
      " [ 38189 159945]]\n",
      "Gradient Boosting - TPR: 0.8073, FPR: 0.1826\n"
     ]
    }
   ],
   "source": [
    "print(\"开始训练梯度提升模型...\")\n",
    "gb = GradientBoostingClassifier(random_state=35)\n",
    "# 使用tqdm来显示训练进度\n",
    "for i in tqdm(range(1), desc=\"Gradient Boosting训练进度\", unit=\"batch\"):\n",
    "    gb.fit(x_train_balanced, y_train_balanced)\n",
    "print(\"完成训练梯度提升模型...\")\n",
    "gb_result = gb.predict(x_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, gb_result))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, gb.predict_proba(x_test)[:, 1]))\n",
    "gb_cm = confusion_matrix(y_test, gb_result)\n",
    "print(\"Gradient Boosting confusion_matrix:\")\n",
    "print(gb_cm)\n",
    "gb_tpr, gb_fpr = calculate_tpr_fpr(gb_cm)\n",
    "print(f\"Gradient Boosting - TPR: {gb_tpr:.4f}, FPR: {gb_fpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(gb_cm, annot=True, fmt=\".1f\", linewidths=.5, square=True, cmap='Blues_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title(f'Gradient Boosting Accuracy: {accuracy_score(y_test, gb_result):.4f}')\n",
    "plt.savefig(\"gb_heatmap.png\", format=\"png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "A05upwjmuug2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练ANN模型...\n",
      "交叉验证得分: [-0.50122249 -0.52322738 -0.40342298 -0.39853301 -0.43627451]\n",
      "平均得分: -0.4525360755549164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANN训练进度:   4%|▍         | 4/100 [00:00<00:12,  7.91epoch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANN训练进度: 100%|██████████| 100/100 [00:13<00:00,  7.52epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成训练ANN模型...\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.21      0.08       438\n",
      "           1       1.00      0.99      0.99    198134\n",
      "\n",
      "    accuracy                           0.99    198572\n",
      "   macro avg       0.52      0.60      0.54    198572\n",
      "weighted avg       1.00      0.99      0.99    198572\n",
      "\n",
      "ROC-AUC Score: 0.6984899995957718\n",
      "ANN confusion_matrix is:\n",
      "[[    93    345]\n",
      " [  1863 196271]]\n",
      "ANN - TPR: 0.9906, FPR: 0.7877\n"
     ]
    }
   ],
   "source": [
    "print(\"开始训练ANN模型...\")\n",
    "kf = KFold(n_splits = 5, shuffle=True, random_state = 42)\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(64,49),\n",
    "                     random_state=35, learning_rate='invscaling', tol=1e-4, batch_size ='auto')\n",
    "\n",
    "# scores = cross_val_score(clf, x_train_balanced, y_train_balanced, scoring='neg_mean_squared_error', cv = kf)\n",
    "# print(\"交叉验证得分:\", scores)\n",
    "# print(\"平均得分:\", np.mean(scores))\n",
    "\n",
    "n_epochs = 100 # 设定epoch数量\n",
    "losses = []  # 用于记录每个 epoch 的损失值\n",
    "\n",
    "# 使用tqdm显示ANN的训练进度\n",
    "for epoch in tqdm(range(n_epochs), desc=\"ANN训练进度\", unit=\"epoch\"):\n",
    "    clf.fit(x_train_balanced, y_train_balanced)\n",
    "    # 计算训练集的预测概率并计算当前 epoch 的 log_loss\n",
    "    y_pred_prob = clf.predict_proba(x_train_balanced)\n",
    "    epoch_loss = log_loss(y_train_balanced, y_pred_prob)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "print(\"完成训练ANN模型...\")\n",
    "ann_result = clf.predict(x_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, ann_result))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, clf.predict_proba(x_test)[:, 1]))\n",
    "ann_cm=metrics.confusion_matrix(y_test, ann_result)\n",
    "print(\"ANN confusion_matrix is:\")\n",
    "print(ann_cm) #打印混淆矩阵\n",
    "ann_tpr, ann_fpr = calculate_tpr_fpr(ann_cm)\n",
    "print(f\"ANN - TPR: {ann_tpr:.4f}, FPR: {ann_fpr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04psrn-1u9Yj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片已保存\n",
      "Loss vs Epoch 图像已保存\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(ann_cm, annot=True, fmt=\".1f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);\n",
    "plt.savefig(\"ann_heatmap.png\", format=\"png\")\n",
    "print(\"图片已保存\")\n",
    "plt.close()  # 关闭图形，防止占用内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Model ( with a single ANN model / with a stacked model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample, class_weight, shuffle  \n",
    "def get_dataset():\n",
    "    positive_df = pd.read_csv('positive_samples_final.csv')\n",
    "    negative_df = pd.read_csv('non_zero_negatives_final.csv')\n",
    "    data_df = pd.concat([positive_df, negative_df], axis=0)\n",
    "    feature_cols = ['JaccardCoefficient', 'PreferentialAttachment', 'AdamicAdar', 'CommonNeighbors', 'ResourceAllocation']\n",
    "    \n",
    "    X = data_df[feature_cols]\n",
    "    y = data_df['link']\n",
    "    \n",
    "    scaler=MinMaxScaler()\n",
    "    X=scaler.fit_transform(X)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "    # 处理数据不平衡问题\n",
    "    x_train_pos = x_train[y_train == 1]\n",
    "    y_train_pos = y_train[y_train == 1]\n",
    "    x_train_neg = x_train[y_train == 0]\n",
    "    y_train_neg = y_train[y_train == 0] \n",
    "    \n",
    "# 数据平衡操作：对正样本进行随机欠采样\n",
    "    x_train_pos_downsampled, y_train_pos_downsampled = resample(\n",
    "    x_train_pos, y_train_pos,\n",
    "    replace = False,  # 不允许重复采样\n",
    "    n_samples = len(x_train_neg),\n",
    "    random_state = 35\n",
    "    )\n",
    "    # 合并欠采样后的正样本和原始负样本\n",
    "    x_train_balanced = np.vstack((x_train_pos_downsampled, x_train_neg))\n",
    "    y_train_balanced = np.hstack((y_train_pos_downsampled, y_train_neg))\n",
    "    \n",
    "    # df1=pd.DataFrame(x_train_balanced, columns=feature_cols)\n",
    "    # df2=pd.DataFrame(y_train_balanced, columns=['label'])\n",
    "    # train_data=pd.concat([df1, df2], axis=1)\n",
    "    # df3=pd.DataFrame(x_test, columns=feature_cols)\n",
    "    # df4=pd.DataFrame(y_test.values, columns=['label'])\n",
    "    \n",
    "    \n",
    "    return x_train_balanced, x_test, y_train_balanced, y_test, scaler, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y,y_pred):\n",
    "    y= y.cpu().detach().numpy()\n",
    "    # y_pred= y_pred.cpu().detach().numpy()\n",
    "    # 混淆矩阵\n",
    "    conf_matrix = confusion_matrix(y, y_pred)\n",
    "    print(\"Confusion matrix\")\n",
    "    print(conf_matrix)\n",
    "    TP=conf_matrix[1][1]\n",
    "    FN=conf_matrix[1][0]\n",
    "    FP=conf_matrix[0][1]\n",
    "    TN=conf_matrix[0][0]\n",
    "    tpr = TP/(TP+FN)\n",
    "    fpr = FP/(FP+TN)\n",
    "    print(\"tpr:\",tpr)\n",
    "    print(\"fpr:\",fpr)\n",
    "    print(classification_report(y, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, scaler, feature_cols = get_dataset()\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test.values)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(x_train, y_train),shuffle=True,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # 新增：添加一维卷积层\n",
    "        self.fc1 = nn.Linear(5, 25)  # 调整输入维度\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(25, 16)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(16, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 新增：使用一维卷积层\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x) \n",
    "        x = self.dropout(x)      \n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet(5).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.1, 1.0]).to(device))\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:02<00:00, 12.52s/it]\n"
     ]
    }
   ],
   "source": [
    "n_models = 5\n",
    "models = []\n",
    "seeds = [1, 35, 21, 5, 7]\n",
    "for i in tqdm(range(n_models)):\n",
    "    \n",
    "    model = SimpleNet(5).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1, 2], device=device,dtype=torch.float32))\n",
    "    x_resampled, y_resampled = resample(x_train, y_train, n_samples=1800,random_state=seeds[i])\n",
    "    tain_loader = DataLoader(TensorDataset(x_resampled, y_resampled),shuffle=False,batch_size=128)\n",
    "    for epoch in range(800):\n",
    "        for batch_idx, (data, target) in enumerate(tain_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[   283    155]\n",
      " [ 26909 171225]]\n",
      "tpr: 0.8641878728537252\n",
      "fpr: 0.3538812785388128\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.65      0.02       438\n",
      "           1       1.00      0.86      0.93    198134\n",
      "\n",
      "    accuracy                           0.86    198572\n",
      "   macro avg       0.50      0.76      0.47    198572\n",
      "weighted avg       1.00      0.86      0.92    198572\n",
      "\n",
      "Confusion matrix\n",
      "[[   259    179]\n",
      " [ 19847 178287]]\n",
      "tpr: 0.8998304177980558\n",
      "fpr: 0.408675799086758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.59      0.03       438\n",
      "           1       1.00      0.90      0.95    198134\n",
      "\n",
      "    accuracy                           0.90    198572\n",
      "   macro avg       0.51      0.75      0.49    198572\n",
      "weighted avg       1.00      0.90      0.94    198572\n",
      "\n",
      "Confusion matrix\n",
      "[[   248    190]\n",
      " [ 16345 181789]]\n",
      "tpr: 0.9175053246792575\n",
      "fpr: 0.4337899543378995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.57      0.03       438\n",
      "           1       1.00      0.92      0.96    198134\n",
      "\n",
      "    accuracy                           0.92    198572\n",
      "   macro avg       0.51      0.74      0.49    198572\n",
      "weighted avg       1.00      0.92      0.95    198572\n",
      "\n",
      "Confusion matrix\n",
      "[[   272    166]\n",
      " [ 24379 173755]]\n",
      "tpr: 0.8769570088929715\n",
      "fpr: 0.3789954337899543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.62      0.02       438\n",
      "           1       1.00      0.88      0.93    198134\n",
      "\n",
      "    accuracy                           0.88    198572\n",
      "   macro avg       0.51      0.75      0.48    198572\n",
      "weighted avg       1.00      0.88      0.93    198572\n",
      "\n",
      "Confusion matrix\n",
      "[[   221    217]\n",
      " [ 11363 186771]]\n",
      "tpr: 0.942649923788951\n",
      "fpr: 0.4954337899543379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.50      0.04       438\n",
      "           1       1.00      0.94      0.97    198134\n",
      "\n",
      "    accuracy                           0.94    198572\n",
      "   macro avg       0.51      0.72      0.50    198572\n",
      "weighted avg       1.00      0.94      0.97    198572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = [model(x_test.to(device)).cpu().detach().numpy() for model in models]\n",
    "for i in range(5):\n",
    "    evaluate(y_test, np.argmax(outputs[i], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[   254    184]\n",
      " [ 20247 177887]]\n",
      "tpr: 0.8978115820606256\n",
      "fpr: 0.4200913242009132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.58      0.02       438\n",
      "           1       1.00      0.90      0.95    198134\n",
      "\n",
      "    accuracy                           0.90    198572\n",
      "   macro avg       0.51      0.74      0.48    198572\n",
      "weighted avg       1.00      0.90      0.94    198572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ensemble_predict(models, X_test):\n",
    "    outputs = [model(X_test).cpu().detach().numpy() for model in models]\n",
    "    avg_outputs = np.mean(outputs, axis=0)  # 对每个模型的输出取平均\n",
    "    return np.argmax(avg_outputs, axis=1)  # 对于分类任务，选择最大值作为最终预测\n",
    "\n",
    "\n",
    "\n",
    "# 获取集成预测\n",
    "predictions = ensemble_predict(models, x_test.to(device))\n",
    "evaluate(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Customized & Stacked Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[   247    191]\n",
      " [ 17840 180294]]\n",
      "tpr: 0.909959926110612\n",
      "fpr: 0.4360730593607306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.56      0.03       438\n",
      "           1       1.00      0.91      0.95    198134\n",
      "\n",
      "    accuracy                           0.91    198572\n",
      "   macro avg       0.51      0.74      0.49    198572\n",
      "weighted avg       1.00      0.91      0.95    198572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def stack_models(models):\n",
    "    train_preds = []\n",
    "    test_preds = []  \n",
    "    for i,model in enumerate(models):\n",
    "        train_output = model(x_train.to(device))\n",
    "        _,train_pred = torch.max(train_output,1)\n",
    "        train_pred = train_pred.cpu().detach().numpy().reshape(-1,1)\n",
    "        \n",
    "        test_output = model(x_test.to(device))\n",
    "        _,test_pred = torch.max(test_output,1)\n",
    "        test_pred = test_pred.cpu().detach().numpy().reshape(-1,1)\n",
    "        \n",
    "        train_preds.append(train_pred)\n",
    "        test_preds.append(test_pred)\n",
    "    train_preds = np.concatenate(train_preds,axis=1)\n",
    "    test_preds = np.concatenate(test_preds,axis=1)\n",
    "    return train_preds,test_preds   \n",
    "\n",
    "train_preds,test_preds = stack_models(models)\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(train_preds,y_train.detach().numpy())\n",
    "y_tets_preds = meta_model.predict(test_preds)\n",
    "\n",
    "evaluate(y_test, y_tets_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- 生成好友推荐功能函数 ---------------\n",
    "def recommend_friends(user1, user2_candidates, base_models, meta_model, scaler, feature_cols):\n",
    "    \"\"\"\n",
    "    为 user1 推荐可能关注的用户。\n",
    "    \n",
    "    Parameters:\n",
    "    - user1: int, 用户ID\n",
    "    - user2_candidates: list of int, 可能的候选用户ID\n",
    "    - base_models: list of nn.Module, 训练好的基模型\n",
    "    - meta_model: sklearn Classifier, 训练好的元模型\n",
    "    - scaler: sklearn.preprocessing, 训练时使用的 scaler\n",
    "    - feature_cols: list of str, 特征列名\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame, 包含 user1, user2, 预测概率的推荐列表\n",
    "    \"\"\"\n",
    "    # 提取所有候选用户对的特征\n",
    "    features_list = []\n",
    "    for user2 in user2_candidates:\n",
    "        features = get_features(user1, user2)\n",
    "        features_list.append(features)\n",
    "    \n",
    "    # 转换为 DataFrame\n",
    "    rec_df = pd.DataFrame(features_list, columns=feature_cols)\n",
    "    \n",
    "    # 归一化特征\n",
    "    X_scaled = scaler.transform(rec_df[feature_cols].values)\n",
    "    X_scaled_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # 获取所有基模型的预测概率\n",
    "    base_preds = []\n",
    "    for model in base_models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(X_scaled_tensor)\n",
    "            probs = torch.softmax(output, dim=1)[:, 1].cpu().numpy()  # 类别1的概率\n",
    "            base_preds.append(probs)\n",
    "    \n",
    "    # 将基模型的预测概率作为元模型的输入特征\n",
    "    meta_features = np.column_stack(base_preds)\n",
    "    \n",
    "    # 使用元模型进行最终预测\n",
    "    final_probs = meta_model.predict_proba(meta_features)[:, 1]  # 类别1的概率\n",
    "    \n",
    "    # 生成推荐列表\n",
    "    recommendations = pd.DataFrame({\n",
    "        'user1': user1,\n",
    "        'user2': user2_candidates,\n",
    "        'prob': final_probs\n",
    "    })\n",
    "    \n",
    "    # 按照预测概率降序排序\n",
    "    recommendations_sorted = recommendations.sort_values(by='prob', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return recommendations_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friend Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_friends(user1, user2_candidates, base_models, meta_model, scaler, feature_cols):\n",
    "    \"\"\"\n",
    "    为 user1 推荐可能关注的用户。\n",
    "    \n",
    "    Parameters:\n",
    "    - user1: int, 用户ID\n",
    "    - user2_candidates: list of int, 可能的候选用户ID\n",
    "    - base_models: list of nn.Module, 训练好的基模型\n",
    "    - meta_model: sklearn Classifier, 训练好的元模型\n",
    "    - scaler: sklearn.preprocessing, 训练时使用的 scaler\n",
    "    - feature_cols: list of str, 特征列名\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame, 包含 user1, user2, 预测概率的推荐列表\n",
    "    \"\"\"\n",
    "    # 提取所有候选用户对的特征\n",
    "    features_list = []\n",
    "    for user2 in user2_candidates:\n",
    "        features = get_features(user1, user2)\n",
    "        features_list.append(features)\n",
    "    \n",
    "    # 转换为 DataFrame\n",
    "    rec_df = pd.DataFrame(features_list, columns=feature_cols)\n",
    "    \n",
    "    # 归一化特征\n",
    "    X_scaled = scaler.transform(rec_df[feature_cols].values)\n",
    "    X_scaled_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # 获取所有基模型的预测概率\n",
    "    base_preds = []\n",
    "    for model in base_models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(X_scaled_tensor)\n",
    "            probs = torch.softmax(output, dim=1)[:, 1].cpu().numpy()  # 类别1的概率\n",
    "            base_preds.append(probs)\n",
    "    \n",
    "    # 将基模型的预测概率作为元模型的输入特征\n",
    "    meta_features = np.column_stack(base_preds)\n",
    "    \n",
    "    # 使用元模型进行最终预测\n",
    "    final_probs = meta_model.predict_proba(meta_features)[:, 1]  # 类别1的概率\n",
    "    \n",
    "    # 生成推荐列表\n",
    "    recommendations = pd.DataFrame({\n",
    "        'user1': user1,\n",
    "        'user2': user2_candidates,\n",
    "        'prob': final_probs\n",
    "    })\n",
    "    \n",
    "    # 按照预测概率降序排序\n",
    "    recommendations_sorted = recommendations.sort_values(by='prob', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return recommendations_sorted\n",
    "\n",
    "# 定义 get_features 函数\n",
    "def get_features(user1, user2):\n",
    "    \"\"\"\n",
    "    根据实际数据实现，用于提取用户对的五种特征。\n",
    "    这里假设有一个预先计算好的特征字典或可以从数据库中查询。\n",
    "    \"\"\"\n",
    "    # 示例：返回随机特征（实际需替换为实际特征提取逻辑）\n",
    "    jaccard = random.random()\n",
    "    preferential_attachment = random.random()\n",
    "    adamic_adar = random.random()\n",
    "    common_neighbors = random.randint(0, 10)\n",
    "    resource_allocation = random.random()\n",
    "    return [jaccard, preferential_attachment, adamic_adar, common_neighbors, resource_allocation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user1  user2      prob\n",
      "0       5      8  0.516328\n",
      "1       5      9  0.516328\n",
      "2       5     11  0.516328\n",
      "3       5     16  0.516328\n",
      "4       5     15  0.516328\n",
      "5       5     13  0.516328\n",
      "6       5     20  0.516328\n",
      "7       5     19  0.516328\n",
      "8       5     18  0.516328\n",
      "9       5      7  0.305236\n",
      "10      5     12  0.305184\n",
      "11      5     10  0.254520\n",
      "12      5     17  0.254503\n",
      "13      5     14  0.192739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dikangdai/anaconda3/envs/doh/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 示例使用\n",
    "user1 = 5\n",
    "user2_candidates = [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]  # 示例候选用户ID\n",
    "\n",
    "# 生成好友推荐\n",
    "recommendations = recommend_friends(user1, user2_candidates, models, meta_model, scaler, feature_cols)\n",
    "print(recommendations)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "doh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
